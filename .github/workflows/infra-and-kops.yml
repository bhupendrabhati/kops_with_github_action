name: infra-and-kops

# Triggers:
# - run plan on pull requests (opened/synchronize/reopened)
# - run apply only on pushes to main
# Skip runs if only workflow files changed
on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths-ignore:
      - '.github/workflows/**'
  push:
    branches:
      - main
    paths-ignore:
      - '.github/workflows/**'
  workflow_dispatch: {} # allow manual runs too

env:
  TF_DIR: infra-kops

permissions:
  contents: read
  id-token: write

jobs:

  # 1) Terraform plan job — runs for pull_request events (PRs)
  terraform-plan:
    name: Terraform Plan (PR)
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_wrapper: false

      - name: Configure AWS credentials (for plan that may need provider info)
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Terraform fmt check
        run: terraform -chdir=${{ env.TF_DIR }} fmt -check || true

      - name: Terraform init
        run: terraform -chdir=${{ env.TF_DIR }} init -input=false

      - name: Terraform validate
        run: terraform -chdir=${{ env.TF_DIR }} validate || true

      - name: Terraform plan (save plan artifact)
        run: |
          set -euo pipefail
          cd "${{ env.TF_DIR }}"
          terraform plan -out=tfplan -input=false
          terraform show -json tfplan > tfplan.json
        shell: bash

      - name: Upload plan artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfplan
          path: |
            ${{ env.TF_DIR }}/tfplan
            ${{ env.TF_DIR }}/tfplan.json

  # 2) Terraform apply job — runs only on pushes to main (or manual dispatch)
  terraform-apply:
    name: Terraform Apply (main)
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    needs: [terraform-plan] # plan job may not run for direct pushes, but that's OK
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Try to download tfplan artifact (if a plan job ran)
        uses: actions/download-artifact@v4
        with:
          name: tfplan
          path: ${{ env.TF_DIR }}
        continue-on-error: true

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_wrapper: false

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Terraform init
        run: terraform -chdir=${{ env.TF_DIR }} init -input=false

      - name: Terraform apply (use plan if present)
        id: tf_apply
        run: |
          set -euo pipefail
          cd "${{ env.TF_DIR }}"
          if [ -f ./tfplan ]; then
            echo "Applying saved plan (tfplan)..."
            terraform apply -input=false -auto-approve ./tfplan
          else
            echo "No saved plan found — running terraform apply -auto-approve"
            terraform apply -auto-approve -input=false
          fi
          # save outputs for next job
          terraform output -json > ../tf_outputs.json
          # mark that apply succeeded (marker file in workspace)
          mkdir -p ../.gha_markers
          echo "apply_success" > ../.gha_markers/apply_success
        shell: bash

      - name: Upload TF outputs artifact
        uses: actions/upload-artifact@v4
        with:
          name: tf-outputs
          path: tf_outputs.json

  # 3) kOps create/update + nginx deploy — runs after apply
  kops-deploy:
    name: kOps cluster create/update & deploy
    needs: terraform-apply
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download TF outputs
        uses: actions/download-artifact@v4
        with:
          name: tf-outputs
          path: .

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install jq, kops and kubectl
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq
          curl -Lo kops https://github.com/kubernetes/kops/releases/latest/download/kops-linux-amd64
          chmod +x kops && sudo mv kops /usr/local/bin/
          curl -Lo kubectl "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          kops version && kubectl version --client
        shell: bash

      - name: Read kops_state and cluster name from TF outputs
        id: tf
        run: |
          if [ ! -f ./tf_outputs.json ]; then
            echo "ERROR: tf_outputs.json not found"; exit 1
          fi
          KOPS_STATE_STORE=$(jq -r '.kops_state_bucket.value // empty' tf_outputs.json)
          CLUSTER_FROM_TF=$(jq -r '.kops_cluster_name.value // empty' tf_outputs.json)

          # prefer an explicit secret override if you set CLUSTER_NAME in Secrets; expand safely
          if [ -n "${{ secrets.CLUSTER_NAME }}" ]; then
            CLUSTER_NAME="${{ secrets.CLUSTER_NAME }}"
          else
            CLUSTER_NAME="${CLUSTER_FROM_TF}"
          fi

          echo "KOPS_STATE_STORE=${KOPS_STATE_STORE}" >> $GITHUB_OUTPUT
          echo "CLUSTER_NAME=${CLUSTER_NAME}" >> $GITHUB_OUTPUT
        shell: bash

      - name: Debug detected values
        run: |
          echo "KOPS_STATE_STORE=${{ steps.tf.outputs.KOPS_STATE_STORE }}"
          echo "CLUSTER_NAME=${{ steps.tf.outputs.CLUSTER_NAME }}"
        shell: bash

      - name: Write SSH pubkey if provided (create kops secret) - safe runtime check
        run: |
          # If the secret is set, create the kops SSH secret; otherwise skip gracefully.
          if [ -n "${{ secrets.KOPS_SSH_PUBLIC_KEY }}" ]; then
            echo "Writing provided KOPS_SSH_PUBLIC_KEY to temp file and creating kops secret..."
            echo "${{ secrets.KOPS_SSH_PUBLIC_KEY }}" > /tmp/kops.pub
            export KOPS_STATE_STORE="${{ steps.tf.outputs.KOPS_STATE_STORE }}"
            kops create secret --name "${{ steps.tf.outputs.CLUSTER_NAME }}" sshpublickey admin -i /tmp/kops.pub --state="${KOPS_STATE_STORE}" || true
          else
            echo "No KOPS_SSH_PUBLIC_KEY found in secrets; skipping kops secret creation."
          fi
        env:
          KOPS_STATE_STORE: ${{ steps.tf.outputs.KOPS_STATE_STORE }}
        shell: bash

      - name: Create or update kops cluster (idempotent)
        env:
          KOPS_STATE_STORE: ${{ steps.tf.outputs.KOPS_STATE_STORE }}
          CLUSTER_NAME: ${{ steps.tf.outputs.CLUSTER_NAME }}
        run: |
          set -euo pipefail
          # Substitute environment variables in cluster.yml template
          if [ -f infra-kops/cluster.yml ]; then
            echo "Preparing cluster.yml with environment variables..."
            mkdir -p /tmp/kops-config
            envsubst < infra-kops/cluster.yml > /tmp/kops-config/cluster-prepared.yml
            echo "Applying prepared cluster configuration..."
            kops replace -f /tmp/kops-config/cluster-prepared.yml --state="${KOPS_STATE_STORE}" || true
          fi

          if kops get cluster --name "${CLUSTER_NAME}" --state="${KOPS_STATE_STORE}" > /dev/null 2>&1; then
            echo "Cluster exists; running update..."
            kops update cluster --name "${CLUSTER_NAME}" --state="${KOPS_STATE_STORE}" --yes
          else
            echo "Cluster not found; creating with fallback defaults"
            # adapt zone/node-count/size to your needs
            kops create cluster --name "${CLUSTER_NAME}" --state="${KOPS_STATE_STORE}" --zones "ap-south-1a,ap-south-1b,ap-south-1c" --node-count 2 --node-size t3.small --yes
          fi
        shell: bash

      - name: Export kubeconfig and validate (non-blocking)
        env:
          KOPS_STATE_STORE: ${{ steps.tf.outputs.KOPS_STATE_STORE }}
          CLUSTER_NAME: ${{ steps.tf.outputs.CLUSTER_NAME }}
        run: |
          set -euo pipefail
          kops export kubeconfig --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}"
          kubectl get nodes --no-headers || true
          # validation can take time — log but don't fail the job if not ready
          kops validate cluster --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" || true
        shell: bash

      - name: Deploy nginx manifests (idempotent)
        run: |
          kubectl apply -f infra-kops/nginx-deploy.yaml || true
          kubectl apply -f infra-kops/nginx-svc.yaml || true
          # wait for rollout if deployment is named 'nginx-demo'
          kubectl rollout status deploy/nginx-demo -n default --timeout=120s || true
        shell: bash

      # mark overall job success (so that cleanup step won't destroy resources)
      - name: Mark job success
        if: ${{ success() }}
        run: |
          mkdir -p .gha_markers
          echo "job_success" > .gha_markers/job_success
          ls -la .gha_markers || true

      # FINAL CLEANUP: run always, and destroy infra if apply succeeded but job did not succeed
      - name: Cleanup on failure — destroy infra & kOps cluster (best-effort)
        if: ${{ always() }}
        env:
          TF_DIR: ${{ env.TF_DIR }}
        run: |
          set -euxo pipefail
          MARKER_DIR=".gha_markers"
          APPLY_MARKER="${MARKER_DIR}/apply_success"
          JOB_MARKER="${MARKER_DIR}/job_success"

          # If apply marker not present, nothing to destroy
          if [ ! -f "${APPLY_MARKER}" ]; then
            echo "No apply marker found — nothing to destroy."
            exit 0
          fi

          # If markers show job succeeded, skip destroy
          if [ -f "${JOB_MARKER}" ]; then
            echo "Job succeeded; skipping cleanup destroy."
            exit 0
          fi

          echo "Detected failed job after apply — running best-effort cleanup."

          # read tf_outputs.json if available to get KOPS state and cluster name
          if [ -f ./tf_outputs.json ]; then
            KOPS_STATE_STORE=$(jq -r '.kops_state_bucket.value // empty' ./tf_outputs.json || true)
            CLUSTER_NAME=$(jq -r '.kops_cluster_name.value // empty' ./tf_outputs.json || true)
            echo "Detected KOPS_STATE_STORE='${KOPS_STATE_STORE}' CLUSTER_NAME='${CLUSTER_NAME}'"
          else
            echo "tf_outputs.json not found; attempting to derive from terraform state (best-effort)"
            # attempt to init and read outputs from TF_DIR
            terraform -chdir="${TF_DIR}" init -input=false || true
            terraform -chdir="${TF_DIR}" output -json > /tmp/tf_out.json || true
            if [ -f /tmp/tf_out.json ]; then
              KOPS_STATE_STORE=$(jq -r '.kops_state_bucket.value // empty' /tmp/tf_out.json || true)
              CLUSTER_NAME=$(jq -r '.kops_cluster_name.value // empty' /tmp/tf_out.json || true)
              echo "Derived KOPS_STATE_STORE='${KOPS_STATE_STORE}' CLUSTER_NAME='${CLUSTER_NAME}'"
            fi
          fi

          # Try deleting kOps cluster (best-effort)
          if [ -n "${KOPS_STATE_STORE:-}" ] && [ -n "${CLUSTER_NAME:-}" ]; then
            echo "Attempting to delete kops cluster ${CLUSTER_NAME} in state store ${KOPS_STATE_STORE} ..."
            # install kops if missing
            if ! command -v kops >/dev/null 2>&1; then
              curl -Lo kops https://github.com/kubernetes/kops/releases/latest/download/kops-linux-amd64
              chmod +x kops && sudo mv kops /usr/local/bin/
            fi
            kops delete cluster --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" --yes || echo "kops delete failed (best-effort)"
            # attempt to remove kops state objects from S3 (best-effort)
            if echo "${KOPS_STATE_STORE}" | grep -q '^s3://'; then
              BUCKET=$(echo "${KOPS_STATE_STORE}" | sed 's|s3://||; s|/.*||')
              aws s3 rm --recursive "s3://${BUCKET}/${CLUSTER_NAME}" || echo "Failed to remove kops state objects (best-effort)"
            fi
          else
            echo "No kOps state or cluster name detected; skipping kOps delete."
          fi

          # Now destroy Terraform-managed infra in TF_DIR
          echo "Running terraform destroy in ${TF_DIR} (best-effort)"
          terraform -chdir="${TF_DIR}" plan -destroy -out=destroy.plan -input=false || true
          if [ -f "${TF_DIR}/destroy.plan" ]; then
            terraform -chdir="${TF_DIR}" apply -auto-approve destroy.plan || terraform -chdir="${TF_DIR}" destroy -auto-approve || echo "Terraform destroy failed (best-effort)"
          else
            terraform -chdir="${TF_DIR}" destroy -auto-approve || echo "Terraform destroy failed (best-effort)"
          fi

          echo "Cleanup finished."