name: infra-and-kops

# Triggers:
# - run plan on pull requests (opened/synchronize/reopened)
# - run apply only on pushes to main
# Skip runs if only workflow files changed
on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths-ignore:
      - '.github/workflows/**'
  push:
    branches:
      - main
    paths-ignore:
      - '.github/workflows/**'
  workflow_dispatch: {} # allow manual runs too

env:
  TF_DIR: infra-kops

permissions:
  contents: read
  id-token: write

jobs:

  # 1) Terraform plan job — runs for pull_request events (PRs)
  terraform-plan:
    name: Terraform Plan (PR)
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_wrapper: false

      - name: Configure AWS credentials (for plan that may need provider info)
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Terraform fmt check
        run: terraform -chdir=${{ env.TF_DIR }} fmt -check || true

      - name: Terraform init
        run: terraform -chdir=${{ env.TF_DIR }} init -input=false

      - name: Terraform validate
        run: terraform -chdir=${{ env.TF_DIR }} validate || true

      - name: Terraform plan (save plan artifact)
        run: |
          set -euo pipefail
          cd "${{ env.TF_DIR }}"
          terraform plan -out=tfplan -input=false
          terraform show -json tfplan > tfplan.json
        shell: bash

      - name: Upload plan artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfplan
          path: |
            ${{ env.TF_DIR }}/tfplan
            ${{ env.TF_DIR }}/tfplan.json

  # 2) Terraform apply job — runs only on pushes to main (or manual dispatch)
  terraform-apply:
    name: Terraform Apply (main)
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    # note: not requiring the plan job so manual dispatch works
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Try to download tfplan artifact (if a plan job ran)
        uses: actions/download-artifact@v4
        with:
          name: tfplan
          path: ${{ env.TF_DIR }}
        continue-on-error: true

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_wrapper: false

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Terraform init
        run: terraform -chdir=${{ env.TF_DIR }} init -input=false

      - name: Terraform apply (use plan if present) — with immediate cleanup on apply failure
        id: tf_apply
        run: |
          set -euo pipefail
          cd "${{ env.TF_DIR }}"

          # marker files (in repo workspace, not committed)
          MARKER_DIR="../.gha_markers"
          APPLY_ATTEMPT="${MARKER_DIR}/apply_attempt"
          APPLY_SUCCESS="${MARKER_DIR}/apply_success"
          mkdir -p "${MARKER_DIR}"
          touch "${APPLY_ATTEMPT}"

          # run apply (use saved plan if present)
          APPLY_EXIT=0
          if [ -f ./tfplan ]; then
            echo "Applying saved plan (tfplan)..."
            terraform apply -input=false -auto-approve ./tfplan || APPLY_EXIT=$?
          else
            echo "No saved plan found — running terraform apply -auto-approve"
            terraform apply -auto-approve -input=false || APPLY_EXIT=$?
          fi

          # If apply failed, check for partial state and destroy resources if present
          if [ "${APPLY_EXIT}" -ne 0 ]; then
            echo "Terraform apply failed with exit code ${APPLY_EXIT}."
            echo "Checking terraform state for created resources (best-effort)..."
            # attempt to list resources known to state
            terraform state list > /tmp/tf_state_list 2>/dev/null || true
            if [ -s /tmp/tf_state_list ]; then
              echo "Resources are present in terraform state (listing first 50):"
              head -n 50 /tmp/tf_state_list || true
              echo "Attempting to destroy created resources to avoid orphaned infra..."
              # create a destroy plan and apply it (best-effort)
              terraform plan -destroy -out=destroy.plan -input=false || true
              if [ -f destroy.plan ]; then
                terraform apply -auto-approve destroy.plan || terraform destroy -auto-approve || true
              else
                terraform destroy -auto-approve || true
              fi
              echo "Best-effort destroy completed (if possible)."
            else
              echo "No resources found in terraform state — nothing to destroy."
            fi

            # upload any partial outputs or state for debugging (optional)
            terraform output -json > ../tf_outputs_partial.json || true

            # fail the step (so the job shows failure) while resources already cleaned (best-effort)
            exit ${APPLY_EXIT}
          fi

          # On success: save outputs and mark apply success
          terraform output -json > ../tf_outputs.json
          echo "apply_success" > "${APPLY_SUCCESS}"
        shell: bash

      - name: Upload TF outputs artifact
        uses: actions/upload-artifact@v4
        with:
          name: tf-outputs
          path: tf_outputs.json

  # 3) kOps create/update + nginx deploy — runs after apply
  kops-deploy:
    name: kOps cluster create/update & deploy
    needs: terraform-apply
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download TF outputs
        uses: actions/download-artifact@v4
        with:
          name: tf-outputs
          path: .

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install jq, kops and kubectl
        run: |
          set -euo pipefail

          sudo apt-get update -y
          sudo apt-get install -y jq unzip ca-certificates

          # --- install kops (follow redirects, verify binary signature lightly) ---
          KOPS_URL="https://github.com/kubernetes/kops/releases/latest/download/kops-linux-amd64"
          echo "Downloading kops from ${KOPS_URL}"
          curl -sSL -o /tmp/kops.bin "${KOPS_URL}"
          # quick sanity check: file should be an ELF binary
          file /tmp/kops.bin || true
          # make executable and move into PATH
          chmod +x /tmp/kops.bin
          sudo mv /tmp/kops.bin /usr/local/bin/kops

          # verify it's executable
          if ! command -v kops >/dev/null 2>&1; then
            echo "kops not found after install"
            ls -l /usr/local/bin/kops || true
            tail -n +1 /usr/local/bin/kops | sed -n '1,4p' || true
            exit 1
          fi
          kops version || true

          # --- install kubectl (stable release) ---
          KUBE_VER=$(curl -L -s https://dl.k8s.io/release/stable.txt)
          echo "Downloading kubectl ${KUBE_VER}"
          curl -sSL -o /tmp/kubectl "https://dl.k8s.io/release/${KUBE_VER}/bin/linux/amd64/kubectl"
          chmod +x /tmp/kubectl
          sudo mv /tmp/kubectl /usr/local/bin/kubectl
          kubectl version --client || true

        shell: bash


      - name: Read kops_state and cluster name from TF outputs
        id: tf
        run: |
          if [ ! -f ./tf_outputs.json ]; then
            echo "ERROR: tf_outputs.json not found"; exit 1
          fi
          KOPS_STATE_STORE=$(jq -r '.kops_state_bucket.value // empty' tf_outputs.json)
          CLUSTER_FROM_TF=$(jq -r '.kops_cluster_name.value // empty' tf_outputs.json)

          # prefer an explicit secret override if you set CLUSTER_NAME in Secrets; expand safely
          if [ -n "${{ secrets.CLUSTER_NAME }}" ]; then
            CLUSTER_NAME="${{ secrets.CLUSTER_NAME }}"
          else
            CLUSTER_NAME="${CLUSTER_FROM_TF}"
          fi

          echo "KOPS_STATE_STORE=${KOPS_STATE_STORE}" >> $GITHUB_OUTPUT
          echo "CLUSTER_NAME=${CLUSTER_NAME}" >> $GITHUB_OUTPUT
        shell: bash

      - name: Debug detected values
        run: |
          echo "KOPS_STATE_STORE=${{ steps.tf.outputs.KOPS_STATE_STORE }}"
          echo "CLUSTER_NAME=${{ steps.tf.outputs.CLUSTER_NAME }}"
        shell: bash

      - name: Write SSH pubkey if provided (create kops secret) - safe runtime check
        run: |
          # If the secret is set, create the kops SSH secret; otherwise skip gracefully.
          if [ -n "${{ secrets.KOPS_SSH_PUBLIC_KEY }}" ]; then
            echo "Writing provided KOPS_SSH_PUBLIC_KEY to temp file and creating kops secret..."
            echo "${{ secrets.KOPS_SSH_PUBLIC_KEY }}" > /tmp/kops.pub
            export KOPS_STATE_STORE="${{ steps.tf.outputs.KOPS_STATE_STORE }}"
            kops create secret --name "${{ steps.tf.outputs.CLUSTER_NAME }}" sshpublickey admin -i /tmp/kops.pub --state="${KOPS_STATE_STORE}" || true
          else
            echo "No KOPS_SSH_PUBLIC_KEY found in secrets; skipping kops secret creation."
          fi
        env:
          KOPS_STATE_STORE: ${{ steps.tf.outputs.KOPS_STATE_STORE }}
        shell: bash

      - name: Create or update kops cluster (idempotent, robust)
        env:
          KOPS_STATE_STORE: ${{ steps.tf.outputs.KOPS_STATE_STORE }}
          CLUSTER_NAME: ${{ steps.tf.outputs.CLUSTER_NAME }}
        run: |
          set -euo pipefail

          # Prepare cluster.yml if present
          if [ -f infra-kops/cluster.yml ]; then
            echo "Preparing cluster.yml with environment variables..."
            mkdir -p /tmp/kops-config
            envsubst < infra-kops/cluster.yml > /tmp/kops-config/cluster-prepared.yml
            echo "Prepared: /tmp/kops-config/cluster-prepared.yml"
            # show first lines for debugging
            sed -n '1,120p' /tmp/kops-config/cluster-prepared.yml || true
          fi

          # Try to apply the prepared config with 'replace' (non-fatal)
          if [ -f /tmp/kops-config/cluster-prepared.yml ]; then
            echo "Attempting kops replace -f (non-fatal if parsing fails)..."
            # If parsing fails, kops returns non-zero; do not abort the script here.
            kops replace -f /tmp/kops-config/cluster-prepared.yml --state="${KOPS_STATE_STORE}" || echo "kops replace produced an error (continuing)"
          fi

          # Determine whether cluster exists already
          if kops get cluster --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" > /dev/null 2>&1; then
            echo "Cluster ${CLUSTER_NAME} already exists in state store; running update..."
            kops update cluster --name "${CLUSTER_NAME}" --state="${KOPS_STATE_STORE}" --yes
            CREATED=false
          else
            echo "Cluster ${CLUSTER_NAME} not found in state store; attempting creation."

            # If we have a prepared cluster YAML, attempt create from file (preferred).
            if [ -f /tmp/kops-config/cluster-prepared.yml ]; then
              echo "Trying to create cluster from prepared cluster.yml (explicit --state and --cloud aws)."
              # Attempt create from file first; if it fails, fall back to 'kops create cluster' with flags.
              if kops create -f /tmp/kops-config/cluster-prepared.yml --state="${KOPS_STATE_STORE}" --cloud aws; then
                echo "Created cluster from YAML."
              else
                echo "Create from YAML failed; falling back to 'kops create cluster' CLI defaults."
                kops create cluster --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" --zones "ap-south-1a,ap-south-1b,ap-south-1c" --node-count 2 --node-size t3.small --yes --cloud aws
              fi
            else
              # No cluster.yml present — create with CLI flags and explicit cloud.
              kops create cluster --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" --zones "ap-south-1a,ap-south-1b,ap-south-1c" --node-count 2 --node-size t3.small --yes --cloud aws
            fi
            CREATED=true
          fi

          # Export kubeconfig (best-effort)
          echo "Exporting kubeconfig (best-effort)..."
          kops export kubeconfig --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" || echo "kops export kubeconfig failed (continuing)"

          # If an SSH pubkey is provided, create the kops secret now (only after cluster exists)
          if [ -n "${{ secrets.KOPS_SSH_PUBLIC_KEY }}" ]; then
            echo "Creating kops SSH secret from provided secret..."
            echo "${{ secrets.KOPS_SSH_PUBLIC_KEY }}" > /tmp/kops.pub
            kops create secret --name "${CLUSTER_NAME}" sshpublickey admin -i /tmp/kops.pub --state="${KOPS_STATE_STORE}" || echo "kops create secret failed (continuing)"
          else
            echo "No KOPS_SSH_PUBLIC_KEY provided; skipping SSH secret creation."
          fi

          echo "Create/update step completed. CREATED=${CREATED}"
        shell: bash


      - name: Export kubeconfig and validate (non-blocking)
        env:
          KOPS_STATE_STORE: ${{ steps.tf.outputs.KOPS_STATE_STORE }}
          CLUSTER_NAME: ${{ steps.tf.outputs.CLUSTER_NAME }}
        run: |
          set -euo pipefail
          kops export kubeconfig --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}"
          kubectl get nodes --no-headers || true
          # validation can take time — log but don't fail the job if not ready
          kops validate cluster --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" || true
        shell: bash

      - name: Deploy nginx manifests (idempotent)
        run: |
          kubectl apply -f infra-kops/nginx-deploy.yaml || true
          kubectl apply -f infra-kops/nginx-svc.yaml || true
          # wait for rollout if deployment is named 'nginx-demo'
          kubectl rollout status deploy/nginx-demo -n default --timeout=120s || true
        shell: bash

      # mark overall job success (so that cleanup step won't destroy resources)
      - name: Mark job success
        if: ${{ success() }}
        run: |
          mkdir -p .gha_markers
          echo "job_success" > .gha_markers/job_success
          ls -la .gha_markers || true

      # FINAL CLEANUP: run always, and destroy infra if apply succeeded but job did not succeed
      - name: Cleanup on failure — destroy infra & kOps cluster (best-effort)
        if: ${{ always() }}
        env:
          TF_DIR: ${{ env.TF_DIR }}
        run: |
          set -euxo pipefail
          MARKER_DIR=".gha_markers"
          APPLY_MARKER="${MARKER_DIR}/apply_success"
          JOB_MARKER="${MARKER_DIR}/job_success"

          # If apply marker not present, nothing to destroy
          if [ ! -f "${APPLY_MARKER}" ]; then
            echo "No apply marker found — nothing to destroy."
            exit 0
          fi

          # If markers show job succeeded, skip destroy
          if [ -f "${JOB_MARKER}" ]; then
            echo "Job succeeded; skipping cleanup destroy."
            exit 0
          fi

          echo "Detected failed job after apply — running best-effort cleanup."

          # read tf_outputs.json if available to get KOPS state and cluster name
          if [ -f ./tf_outputs.json ]; then
            KOPS_STATE_STORE=$(jq -r '.kops_state_bucket.value // empty' ./tf_outputs.json || true)
            CLUSTER_NAME=$(jq -r '.kops_cluster_name.value // empty' ./tf_outputs.json || true)
            echo "Detected KOPS_STATE_STORE='${KOPS_STATE_STORE}' CLUSTER_NAME='${CLUSTER_NAME}'"
          else
            echo "tf_outputs.json not found; attempting to derive from terraform state (best-effort)"
            # attempt to init and read outputs from TF_DIR
            terraform -chdir="${TF_DIR}" init -input=false || true
            terraform -chdir="${TF_DIR}" output -json > /tmp/tf_out.json || true
            if [ -f /tmp/tf_out.json ]; then
              KOPS_STATE_STORE=$(jq -r '.kops_state_bucket.value // empty' /tmp/tf_out.json || true)
              CLUSTER_NAME=$(jq -r '.kops_cluster_name.value // empty' /tmp/tf_out.json || true)
              echo "Derived KOPS_STATE_STORE='${KOPS_STATE_STORE}' CLUSTER_NAME='${CLUSTER_NAME}'"
            fi
          fi

          # Try deleting kOps cluster (best-effort)
          if [ -n "${KOPS_STATE_STORE:-}" ] && [ -n "${CLUSTER_NAME:-}" ]; then
            echo "Attempting to delete kops cluster ${CLUSTER_NAME} in state store ${KOPS_STATE_STORE} ..."
            # install kops if missing
            if ! command -v kops >/dev/null 2>&1; then
              curl -Lo kops https://github.com/kubernetes/kops/releases/latest/download/kops-linux-amd64
              chmod +x kops && sudo mv kops /usr/local/bin/
            fi
            kops delete cluster --name "${CLUSTER_NAME}" --state "${KOPS_STATE_STORE}" --yes || echo "kops delete failed (best-effort)"
            # attempt to remove kops state objects from S3 (best-effort)
            if echo "${KOPS_STATE_STORE}" | grep -q '^s3://'; then
              BUCKET=$(echo "${KOPS_STATE_STORE}" | sed 's|s3://||; s|/.*||')
              aws s3 rm --recursive "s3://${BUCKET}/${CLUSTER_NAME}" || echo "Failed to remove kops state objects (best-effort)"
            fi
          else
            echo "No kOps state or cluster name detected; skipping kOps delete."
          fi

          # Now destroy Terraform-managed infra in TF_DIR
          echo "Running terraform destroy in ${TF_DIR} (best-effort)"
          terraform -chdir="${TF_DIR}" plan -destroy -out=destroy.plan -input=false || true
          if [ -f "${TF_DIR}/destroy.plan" ]; then
            terraform -chdir="${TF_DIR}" apply -auto-approve destroy.plan || terraform -chdir="${TF_DIR}" destroy -auto-approve || echo "Terraform destroy failed (best-effort)"
          else
            terraform -chdir="${TF_DIR}" destroy -auto-approve || echo "Terraform destroy failed (best-effort)"
          fi

          echo "Cleanup finished."
